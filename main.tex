\documentclass[10pt,conference]{ieeeconf}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}


\begin{document}


% Artificial potential fields

% Combinatorial methods

% Sampling
%\nocite{PRM}
%\nocite{RRT}

% RAMP
%\nocite{EPN_Adaptive}
%\nocite{RAMP}


\author{Sterling McLeod\\Department of Computer Science, UNC Charlotte}
\title {Robot Motion Planning\\PhD Qualifying Exam Written Component}
\date {2016}

\maketitle

\renewcommand{\algorithmicforall}{\textbf{for each}}

\section{Introduction}

	Robot Motion Planning is the problem of moving a robot from an initial state to a goal state in an environment that may contain obstacles. States can be planar positions, or they can be high-dimensional configurations. They can also include dynamic information, such as a specific velocity and/or acceleration.	Motion planning is required in almost every robotics tasks that exists. Unfortunately, this problem is made difficult by several aspects: 
	
	\begin{itemize}
	\item \emph{Kinematic structure of a robot}
	
	Most path-planning algorithms will operate in a robot's Configuration Space (Section \ref{sec:C-space}). The number of dimensions in this space scales with the degrees of freedom in a robot's kinematic structure. Mobile bases will only have 2-3 dimensional C-spaces, but manipulators can have up to 7. Humanoid robots can have over 20. Finding a path in such high-dimensional spaces is extremely computationally expensive. 
	
	\item \emph{Complexity of an environment}
	
	Environment complexity is defined by the obstacles existing in it. If all obstacles are static, then the planning will not be too difficult. If some or all of the obstacles are dynamic, then the planning is made much more complicated because the representation of an environment created at one time may not be accurate for future times. Some planning algorithms build graphs that assume the obstacle are static. If the obstacles are dynamic, then the algorithms must rebuild a graph representing the state space. Rebuilding this space can be too computationally expensive to execute at real-time. 
	
	%The dimensionality of an environment is also an important factor. Generally, mobile bases operate in a 2D environment. Manipulators and humanoids, however, operate in a 3D environment. The added dimension brings many more computational costs when performing collision detecton.
	
	\item \emph{Amount of obstacle information}
	
	The inherent complexity of dynamic obstacles can be diminished if a planner knows useful information about the obstacles. If we know exactly how an obstacle will move, then a planner can know where it will be in the future and plan accordingly. Even partial information about obstacle movement would significantly reduce the planning problem complexity because it allows a system to make assumptions. For instance, in the domain of self-driving cars, an assumption can be made that the surrounding cars will only move forward. This means that a planner does not have to consider the case where an obstacle may move directly at the robot. Other domains, such as warehouse robots, cannot have this assumption applied. In order to have a general planner, one must assume as little as possible about obstacles and the algorithm must be developed to handle lots of various situations.
	

	
	\end{itemize}
	
	%The path-planning work is normally carried out in a robot's Configuration Space (Section \ref{sec:C-space}) to simplify making connections from one state to another. 
	
	The earliest works revolve around complete planners operating on a roadmap of the robot's state space. However, building such a roadmap in a high-dimensional space is extremely computationally expensive. Artificial Potential Fields came about as an attempt to move a robot without computing such roadmaps. However, these methods still require an exact representation of a robot's C-space. Sampling-based algorithms gave roboticists a way to build an approximation of a robot's C-space, which made many high-dimensional planning problems feasible to solve.

	
\section{Configuration Space}\label{sec:C-space}

A robot's \emph{Configuration Space}, or C-space, is a manifold mapping the set of all transformations that can be applied to a robot to Euclidean space. The dimensionality of this space is determined by the number of degrees of freedom in a robotic system. A robot's \emph{configuration} is a complete specification of the positions of a robot's points. It is typically represented as a vector specifying the values for each of a robot's degrees of freedom. Each point in the C-space is a vector specifying a transformation of the robot. A more intuitive way to think of C-space is as the set of all possible configurations that a robot can have.

In order to move a robot's end-effector, actuators have to produce rotations or translation around the joints of a robot. This means that any motion of a robot is entirely controlled by the motion of its joints, or the change in configuration. Therefore, robot motion planning is concerned with how to move a robot from an initial configuration to a goal configuration.
    
    To define the C-space for a robot more formally, both the range and type of motion of the robot must be considered. If a robot is a rigid-body object translating in a plane, then its transformation can be expressed in terms of $x$ and $y$. This yields a manifold $M=\mathbb{R}^2$. If rotations can be applied to the robot, then another dimension must be added to the C-space manifold. Let $\theta=[0, 2\pi]$ be the range of rotation for the robot. The C-space manifold for a rigid body robot that can translate along $x$ and  $y$ and rotate $[0, 2\pi]$ in a plane becomes $M=\mathbb{R}^2 \times \mathbb{S}^1$.
    
    For rigid bodies moving in a 3D space, the C-space becomes more complicated. Generally, objects in a 3D space can translate in three dimensions and can rotate about three axes independently. This set of transformations is characterized by a position vector, $p$, and a rotation matrix, $R$, relative to a fixed frame. The resulting C-space will be six-dimensional: $C=\mathbb{R}^3 \times \mathbb{R}\mathbb{P}^3$. 
    
    Manipulators consist of multiple rigid bodies that can either translate or rotate independently of one another. When computing the C-space of articulated bodies, the C-spaces of \emph{each} body must be combined. If a manipulator is a chain of $n$ links connected by revolute joints and each joint can achieve full rotation $\theta_i=[0, 2\pi]$, then the C-space is defined as
    
    \begin{equation}
    C = \mathbb{S}^1 \times \mathbb{S}^1 \times ... \times \mathbb{S}^1 = \mathbb{T}^n
    \end{equation}
   
    Visually, this C-space is an $n$-dimensional torus. If a manipulator can translate in a 3D space, then an additional $\mathbb{R}^3$ must be considered: $C=\mathbb{R}^3 \times \mathbb{T}^n$. In the real world, however, it may not be possible to achieve a full rotation around each joint. If that is the case, then the topological space representing rotation in the C-space would be $\mathbb{R}^n$ instead of $\mathbb{T}^n$. 
    
    The C-space in a motion planning problem is separated into two subspaces, $C_{obs}$ and $C_{free}$. The subspace $C_{obs}$ represents the set of configurations containing obstacle regions and $C_{free} = C \backslash C_{obs}$. Let $A$ be a rigid body robot and $O$ be an obstacle region. Both $A$ and $O$ exist in a world space $W$. The C-space of $A$ is $C$ and a configuration $q \in C$ is defined as $q=(x, y, \theta)$. The obstacle region, $C_{obs}$, is defined as
    
    \begin{equation}
    C_{obs} = \{q \in C | A(q) \cap O \neq \emptyset\}
    \end{equation}

In order to solve a motion planning problem, $C_{obs}$ must be computed or approximated. In the case of a rigid body robot translating in a plane that contains polygonal obstacles, $C_{obs}$ can be found by \emph{growing} the obstacles. For instance, if the robot is a circle robot, all obstacles have each vertex extended to the size of the robot's radius. There is a more systematic way to compute how the obstacles grow if the robot is a convex polygon. The Minkowski Sum is computed for the set of robot and obstacle vertices:

\begin{equation}
A \oplus B = \{(a,b) | a \in A, b \in B\}
\end{equation}

where $A$ is the set of robot vertices and $B$ is the set of obstacle vertices. The convex hull of $A \oplus B$ represents the final C-space obstacle region. To picture this algorithm visually, it is the result of sliding the robot along the obstacle's edges at the robot's reference vertex. For that reason, it is often referred to as the \emph{sliding algorithm}. This sliding process was first proposed in \cite{lozano1979algorithm}. The works \cite{lozano1981automatic} and \cite{lozano1983spatial} show how the sliding algorithm can be extended to 3 dimensions and manipulator models. The overall result is the same (obstacles are grown based on the robot size), however, the time to compute the grown obstacles is significantly increased because this algorithm has complexity $O(n+m)$ where $n$ is the number of robot edges and $m$ is the number of obstacle edges.

Path-planning problems usually operate in a robot's C-space. Since motion planning problems need to obtain a trajectory, some algorithms operate in a robot's Configuration-Time Space (CT-space). A robot's CT-space only differs from its C-space by adding a dimension to represent time.


\section{Problem Formulation} \label{sec:probform}


Suppose a robot $A$ is a rigid-body moving in some world space $W$. Let $Q$ be the Configuration Space of $A$ as defined in the above section. Let $Q_{obs}$ be the obstacle region occupied by obstacle(s) $O$ in $Q$. The free region is defined as $Q_{free} = Q \backslash Q_{obs}$. A motion planning query would require the robot $A$ to move from an initial state, $q_S$, to a goal state, $q_G$. Based on this description, a full robot motion planning problem has the following components:

\begin{itemize}
\item $A$: A rigid-body robot
\item $W$: A world space that a robot moves in
\item $Q$: The Configuration Space of robot $A$
\item $O$: A description of obstacles in order to form $Q_{obs}$ and $Q_{free}$.
\item $q_S$: The initial state of $A$
\item $q_G$: The desired goal state
\end{itemize}
 
These components can be grouped into a tuple: $(A, W, Q, O, q_S, q_G)$. 

%To solve the problem, a path , $\rho[0,1] \rightarrow Q_{free}$, must be generated such that $\rho(0) = q_S$ and $\rho(1) = q_G$. Then, a trajectory-following algorithm can be employed to generate motion to move the robot along the path while satisfying the robot's dynamics constraints. 

%\emph{Kinodynamic Planning} is an extension to this general problem that integrates path-planning and trajectory-following. 

A solution to the problem is a trajectory $\Gamma$ that moves the robot from its initial state to the goal state while avoiding obstacles. Knot points for the trajectory can be generated by a path-planning algorithm that outputs a path $\rho[0,1] \rightarrow Q_{free}$ such that $\rho(0) = q_S$ and $\rho(1) = q_G$. Or, the trajectory can be generated while a path is being generated, which is known as \emph{Kinodynamic Planning} and will be covered in Section \ref{sec:kino}.

This paper focuses on the planning aspect of motion planning because the essence of motion planning is in exploring a state space as rapidly as possible. Trajectory-following  is closer to the field of \emph{control} and is performed only after a state space has been searched to form a path.


\section{Combinatorial Methods}
	
	Combinatorial motion planning methods are complete planning algorithms that construct a roadmap in a known, continuous space in order to search through for path planning queries. These methods compute the exact configuration space for a robot and its environment in order to construct the roadmap. These algorithms work well for simple robotics problems, but does not scale up to many real-world applications seen today. 
	
\subsection{Roadmaps}
	
	Let $G$ be a graph structure of a topological space that maps to a robot's configuration-free space, $C_{free}$. The set $S$ is the set of points reachable in $C_{free}$ by $G$. In order for $G$ to be a roadmap, two conditions must be satisfied:
	\begin{enumerate}
	\item 
	A graph-search algorithm can compute a path from any initial node to any goal node in the space represented by $G$.
	
	\item
	Given initial and goal configurations $q_I$ and $q_G$, respectively, in $C_{free}$, it is possible to connect corresponding nodes $s_1$ and $s_2$ in $S$. More formally, if there exists a path $\tau :[0,1] \rightarrow C_{free}$, such that $\tau (0)=q_I$ and $\tau (1)=q_G$, then there also exists a path $\tau'[0,1] \rightarrow S$, such that $\tau'(0)=s_1$ and $\tau'(1)=s_2$, where $s_1$ and $s_2$ are nodes on the roadmap.
	\end{enumerate}
	
	An important implication of the second condition is that solutions to path-planning queries are not missed due to the roadmap not capturing the connectivity of the configuration space. Essentially, the second condition ensures that algorithms searching on a roadmap can be complete. The first condition makes it always possible to connect an initial and goal configuration to a roadmap. This is a necessary condition because the initial and goal nodes are not connected to the roadmap when it is constructed. They are only connected when beginning a query.
	
\subsection{Cell Decomposition}

The goal of cell decomposition is to partition a topological space into cells that lie entirely in $C_{free}$. From such a partitioning, a roadmap can be constructed in order to reduce the path planning problem to a graph-search problem. There are several choices for decomposition. Some methods are best suited for polygonal spaces, some are better suited for higher-dimensions. However, all cell decompositions must satisfy three properties:

\begin{enumerate}
\item A path from any point in a cell to any point in a separate cell should lie in $C_{free}$.

\item Adjacency information is easy to extract.

\item Given any configuration $q$, calculating which cell $q$ lies in should be simple.
\end{enumerate}

The first condition is the most important. If it is known that the robot can move from one cell to another without encountering $C_{obs}$, then the problem is reduced to finding a path among a small number of cells, i.e. a graph-search problem. The second and third conditions are necessary to make the querying fast.


\subsubsection{Complexes}

A \emph{complex} is defined as a collection of cells and their boundaries. The difference between the terms \emph{cell decomposition} and \emph{complex} is that complexes contain additional pieces of information about how cells fit together, whereas cell decompositions only contain the actual cells themselves. Cell decompositions are derived from complexes.

The type of complex for a space will dictate the cell decomposition method that can be derived for the space. Two types of complexes are described below.

A \emph{simplicial complex} satisfies the following conditions:

\begin{enumerate}
\item Any face of the complex is also in the complex.
\item The intersection of any two simplexes in the complex is a common face of both simplexes
\end{enumerate}

Essentially, all of cells must be connected and share boundaries. 

A \emph{singular complex} is a generalized version of simplicial complexes that can be defined on a manifold of $\mathbb{R}^n$. Singular complexes also satisfy two conditions:

\begin{enumerate}
\item For each dimension $k$, the set $S_k \subseteq X$ must be closed. 
\item Each $k$-cell is an open set in the topological subspace $S_k$. Note that 0-cells are open in $S_0$, even though they are usually closed in $X$.
\end{enumerate} 

\subsubsection{Approaches}

Vertical Cell Decomposition \cite{vert_cell_decomp} partitions the free C-space into triangles and trapezoids. These simple shapes represent the cells of $C_{free}$. 

At each vertex in $C_{obs}$, vertical rays are extended above and below the vertex until another points in $C_{obs}$ is reached. These rays will define edges for the polygonal cells that partition $C_{free}$. The polygons that form $C_{obs}$ can be either convex or concave and the cells in $C_{free}$ will be either triangular or trapezoidal. 

Building a roadmap from the partitioning amounts to choosing sample points from each cell and connecting them to edges of the cell. More formally, a roadmap $G(V,E)$ is defined as follows: For every cell, $C_i$, define an interior point $q_i$. For each vertical edge in $C_i$, define a point $s_i$. Add the points $q_i$ and $s_i$ to $V$. Then, create an edge, $e_i$, between $q_i$ and the sample points on its cell's vertical edges. Add $e_i$ to $E$. The resulting graph is a roadmap that can be used for efficient path planning.

The running time of this algorithm is $O(nlogn)$ when using the \emph{line-sweep} principle. This principle is the idea of a line sweeping across the space and noting where there are interesting points in the space. In this application, the line is vertical and the interesting points are the vertices of obstacles. An \emph{event} is each time the line finds an interesting point. 

Triangulation is an alternative to vertical decomposition for polygonal spaces. The cells formed from triangulation will be, discernibly, triangles, whereas the cells formed from vertical decomposition are a mix of triangles and trapezoids. The running time of a naive approach to triangulation is $O(n^3)$. That is far too long, even for some 3D spaces. However, the running time can be reduced to $O(n^2logn)$ with radial sweeping.

Cylindrical decomposition is similar to vertical decomposition in that it works by extending rays from all the obstacle vertices, but in the cylindrical method the rays do not stop until they reach the boundary of the C-space. This results in cells that alternate between regions of $C_{free}$ and $C_{obs}$. 

\subsubsection{3D Decomposition}

Cell decomposition in 3 (or more) dimensions is desirable since most motion planning problems have a C-space of more than 2 dimensions. The methods discussed previously work well for 2D, but only cylindrical decomposition generalizes well to higher dimensions. Vertical decomposition can be recursively applied to $k$-cells of higher dimensions, but it only succeeds if $C_{obs}$ is represented as a semi-algebraic model with linear primitives. That format for $C_{obs}$ is rare for a configuration space since many high-DOF robot models contain revolute joints.

\subsubsection{Querying}

To solve a path planning query using the roadmap generated by a cell decomposition method, the initial and goal configurations, $q_I$ and $q_G$ respectively, need to be added to the roadmap. The third condition for cell decomposition listed in the previous section comes into play here. It should be simple to determine which cells $q_I$ and $q_G$ are in. Let $C_I$ and $C_G$ denote the cells that contain $q_I$ and $q_G$, respectively. Once $C_0$ and $C_G$ are identified, edges are found between the configurations and the corresponding cell's sample point. Thus, an edge is formed between $q_I$ and the sample point of $C_I$. Similarly, an edge is formed between $q_G$ and the sample point of $C_G$. Both edges are added to the roadmap $G(V,E)$. Graph search algorithms can be applied to the roadmap to find a path from $q_I$ and $q_G$ or find if a path does not exist.


\subsection{Combinatorial Motion Planning Complexity}

The crux of combinatorial methods is that the upper bound on the number of possible paths in a roadmap scales factiorally with the number of edges and vertices in a graph. 

Consider the simpler problem of a robot moving in a directed acyclic grid with dimensions $m \times n$ and each edge is the same length. If the robot is at an inital location $(s_x, s_y)$ and the goal is given as $(g_x, g_y)$, then the total number of solutions is 

\begin{equation}\label{eq:comb_complex}
|P| = { |g_x-s_x|+|g_y-s_y| \choose |g_x-s_x| }
\end{equation}

Given (\ref{eq:comb_complex}), the total number of paths to move from one corner to the opposite corner can be computer as:

\begin{equation}
|P| = {m+n \choose m}
\end{equation}

These equations assume that the robot would never have a cyclic path. This assumption may hold for path planning in static environments, but it is one that cannot be made for the more general problem of path planning in dynamic environments. A robot may need to move back to a previous position due to the environment changing or the robot made need a path with a new direction to avoid an obstacle. Thus, the number of possible paths for a real-time motion planning system would be even more than the directed acyclic graph case. 


Another significant issue with these methods is that the exact C-space must be computed to perform the decomposition methods. Constructing the C-space will scale exponentially with the degrees of freedom of a robot and the number of obstacles in the workspace.


While it is good to understand the theoretical components of these algorithms, it would be far too time-consuming to run any combinatorial method on a real-time motion planning system. 




\section{Artificial Potential Fields}

In an artificial potential field approach \cite{khatib1986real}, a robot is treated like a particle in a gradient field. The goal state emits an attractive force that acts on the robot and the obstacles emit repulsive forces. The combination of these forces results in a vector field that guides the robot from its initial state to the goal state. At each location, a robot's motion is dictated by the magnitude and direction of the gradient vector at the location. This approach integrates high-level path planning with lower-level robot control to achieve both obstacle avoidance and task-consistent motion simultaneously.

Encountering local minima is the largest problem when using this approach. If a robot is led into an area dominated by repulsive forces, then that area will lead the robot to move in a loop. The most common method of escaping local minima is random walks. When it is detected that the robot has encountered a local minima, a random direction is chosen and the robot is moved in that direction for a variable amount of time. After moving in a random direction, it is hoped that the robot will have escaped the local minima region and can resume moving towards the goal.

Another issue with using potential fields to move a robot is that the exact C-space must be computed. As discussed before, this is usually infeasible when the C-space is high-dimensional.

Potential fields inspired the Elastic Strips approach described in Section \ref{sec:elastic} that addresses these problems by using a higher-level planner to maintain global path information, and reduce the necessary parts of the C-space that must be computed to use potential field controllers.




	
\section{Sampling Based Methods}

Computing the exact configuration space in high dimensions becomes extremely expensive. This makes previous methods, such as potential fields and combinatorial roadmaps, inapplicable to motion planning problems of high dimensions, such as a 6-DOF manipulation problem. Sampling-based methods were introduced in order to handle motion planning problems with such high complexity. Sampling algorithms sacrifice completeness and optimality, but obtain higher performance results. 

Sampling based methods do not access the C-space directly. An associated collision detection algorithm will test a the robot at a sampled configuration with obstacles in the environment. Based on the result of collision detection, the sampling algorithm knows if a sample is free or obstacle space. After sampling many points, the result is an approximation of the C-space, rather than a complete description. 

Sampling has become the method of choice for most modern robotics applications. A sampling based method was first proposed in 1996 \cite{PRM} that builds a graph representation of the C-space approximation and then uses graph-search techniques to find a final path. A tree-based approach was introduced shortly after that could take kinodynamic constraints into account \cite{RRT}. These two approaches have been extended and iterated upon many times and remain the top approaches in the field.

\subsection{Probabilistic Roadmaps}

The basic Probabilistic Roadmaps algorithm \cite{PRM} works in two phases: learning and querying. 

\subsubsection{Learning Phase}

The goal of the \emph{learning} phase is to build the C-space approximation. An overview of the learning phase is summarized below.

\begin{algorithm}
\caption{PRM Learning Phase}
\begin{algorithmic}[1]
\State $N: $ number of nodes to include in the roadmap
\State $V \leftarrow \emptyset$ // Initialize set of vertices
\State $E \leftarrow \emptyset$ // Initialize set of edges
\State $i \leftarrow 0$
\While{$i < N$}
\State $q \leftarrow $ randomly sampled configuration
\If{$q \in C_{free}$}
	\State $V \leftarrow V \cup \{q\}$;
	\State $V_c \leftarrow $ neighborhood set of $q$;
	\ForAll{$v \in V_c$}
		\If{$Can\_Connect(v, q)$}
			\State $e \leftarrow $new edge($q,v$)
			\State $E \leftarrow E \cup \{e\}$
		\EndIf
	\EndFor
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

The goal is to build an undirected graph containing $N$ nodes. These nodes correspond the collision-free points in a robot's configuration space. When a point is sampled and determined to be collision-free, a neighborhood of points from the existing graph is found. For each of these neighborhood points, if a local planner can find a collision-free path from the sampled point to the neighborhood points, an edge is formed connecting the nodes.If a sampled node is found to be in collision, the node is thrown away and the algorithm samples again. 

A local planner is utilized in order to create edges within the graph. Designing a local planner can be difficult, however, because it still needs to plan in a high-dimensional space. Cheap and fast planners are possible since we do not need to consider a global objective. Even if fast local planners are not complete, they are desired over more powerful methods because they allow for more connections to be considered during the learning phase. They allow this because if a local planner is fast, then checking connections between many nodes is fast. Thus, the faster a local planner is, the more connections can be made in a small amount of time. It is desirable to sample as many nodes and make as many connections as possible since more nodes and connections will generally lead to a more accurate C-space approximation. For these reasons, local planners that connect nodes via straight lines are commonly used.

Determining the neighborhood of a node is based on the value of a distance function between two nodes. The distance function is closely related to the choice of local planner because the distance should be proportional to the chance that the local planner can connect the two nodes with a collision-free path. For straight line local planners, the distance function can be the Euclidean distance between the nodes' positions in the robot's workspace. For more complicated local planners, however, more involved distance functions must be defined.  

Narrow passages are a critical concern during the learning phase. A narrow passage is a region that is mostly enclosed by obstacles. These regions are of concern because it is difficult to sample inside of them and to find connections to nodes inside the regions. If a robot must pass through a narrow passage to reach its goal, then the learning phase may be prolonged to the point of being impractical. 

The PRM method tries to address narrow passages by performing an \emph{expansion} step after constructing the graph, in order to improve the graph's connectivity. To accomplish that, nodes that lie in narrow passages are selected and an attempt to find neighboring points in $C_f$ is made. 

The difficulties in the expansion step is to identify which nodes lie in a narrow passage (those are the nodes that need expansion) and how to find neighboring points.

To find potential expansion nodes, a heuristic needs to be defined that predicts the likelihood of some node $c$ being unconnected. In \cite{PRM}, the following was used:

\begin{equation}
r_f(c) = \frac{f(c)}{n(c)+1}
\end{equation}

where $n(c)$ is the number of times the planner tried to connect node $c$ and $f(c)$ is the number of successful connects made to $c$. This function is the \emph{failure ratio} of node $c$.  

Random walks are a common method for actually expanding a node. A direction is chosen at random and the direction is followed until an obstacle is reached. An edge is then formed between node $c$ and a new node $n$ that is a node somewhere in between $c$ and the obstacle that was reached in the chosen direction. This process repeats by selecting new random directions for a pre-specified amount of time.

\subsubsection{Query Phase}

The \emph{query} phase searches the roadmap constructed during the learning phase to find a path from an initial configuration to a goal configuration. 

The first step in this phase is to connect the starting configuration, $s$, and goal configuration, $g$, to the roadmap. The local planner used in the learning phase is used for this task. The procedure for connecting $s$ to the roadmap, $R$, begins by identifying the closest node on $R$ to $s$. The local planner attempts to form a path between $s$ and the closest node. If it cannot find a path, the planner uses the next-closest node on $R$ to connect to $s$. This process repeats until a connection is successful. Once $s$ has been connected, the goal configuration $g$ is connected in the same way. Once both $s$ and $g$ are connected to $R$, any graph-search algorithm will return a path from $s$ to $g$ in the robot's configuration space.

\subsubsection{Further Discussion}

The PRM planner is considered \emph{probabilistically} complete. In most cases, it will not explore every point in the C-space. Given enough time, it will explore all points in the C-space and be able to determine if a path does not exist. 

Dynamic environments are difficult to address with the PRM approach. Constructing the roadmap can take significant computation time and is only useful for the state of the environment while constructing the roadmap. If any obstacles move, then the roadmap needs to be modified or completely remade based on the new state of the environment.


\subsection{Rapidly-Exploring Random Trees}

Rapidly-exploring Random Trees (RRT) is a popular sampling based approach to planning in high dimensional spaces \cite{RRT}. The key difference between RRT and PRM is that RRT uses a tree structure to approximate a robot's C-space, rather than the undirected graph structure used in PRM. This approach leads to several attractive features: it is suitable for handling non-holonomic and kinodynamic constraints, it requires few parameters, and is more apt to handle real-time planning in dynamic environments. RRT was specifically designed for kinodynamic planning (discussed more in Section VI), i.e. it considers both configurations and differential constraints. For this reason, it is also referred to as a randomized kinodynamic planner \cite{lavalle2001randomized}.  

The kinodynamic state space contains both the set of the configurations and the set of possible velocities. A single state would represent the robot's velocity at a particular configuration. Let $Q$ denote the configuration space of a rigid or articulated object in space. Each state $q \in Q$ represents a single configuration of the object. For kinodynamic planning, a state $x \in X$ would be defined as $x=(q,\dot{q})$ and $X$ is the set of all such states. Other types of spaces are viable, however. The state space could include higher-order derivatives or only the configuration $q$. If RRT is considering differential constraints, then the output will be a trajectory as opposed to only a geometric path.

A state transition function in the form $\dot{x} = f(x,u)$ must be defined to consider kinodynamic constraints. The inputs are a state, $x$, and an input, $u$, and the output is the derivative of the state over time. This function can be integrated in order to find the next state resulting from applying $u$ to $x$. This allows the algorithm to consider dynamics constraints while planning. 

Pseudocode for building an RRT can be seen below:

\begin{algorithm}
\caption{RRT Overview}
\begin{algorithmic}[1]
\Function {Construct RRT}{$x_{init}$, $x_{goal}$, $\Delta t$} 
\State T.init($x_{init}$)
\While{$x_{goal} \notin T$}
\State $x_{rand}$ = newly sampled state
\State $x_{near}$ = nearest neighbor of $x_{rand}$
\State $u$ = input needed to move from $x_{near}$ to $x_{rand}$
\State $x_{new}$ = $\{x_{near}, u, \Delta t\}$
\State $T.add\_vertex(x_{new})$
\State $T.add\_edge(x_{near}, x_{new}, u)$
\EndWhile
\State Return $T$
\EndFunction
\end{algorithmic}
\end{algorithm}

At each iteration, a new state, $x_{rand}$, is randomly sampled. The parent of $x_{rand}$ will be the vertex on the tree that is nearest to the sample. The control input, $u$, is then selected to move from $x_{near}$ to $x_{rand}$ without moving into any regions occupied by obstacles in the state space. The new state, $x_{new}$ is formed from the nearest neighbor, the control input, and a small time increment. The new node is added as a vertex on the tree and the edge connecting $x_{near}$ to $x_{new}$ by applying $u$ is added to the tree as a new edge. This process repeats until the goal state has been added to the tree. Since every state has only one parent on the RRT, adding the goal state to the tree means that a path has been found. 

The RRT approach has several nice properties. Similar to PRM, the RRT algorithm is probabilistically complete. The chance of determining whether or not a path exists increases as times goes on. As previously discussed, RRT is able to consider kinodynamic constraints. This is largely due to using a tree over an undirected graph. Since each state will have only one directed connection, it is easy to ensure that a robot's dynamic constraints will be satisfied as it move from configuration to configuration. Another attractive property is that the tree will always remain connected, even in narrow passages. Graphs constructed with the PRM are not guaranteed to remain connected because the connections are based on a vertex's neighborhood. If there are no existing vertices nearby the new vertex, then the vertex will have no connections, but will still be added to the graph.

One of the most appealing aspects of RRT is that it is bias towards unexplored areas of a space. This occurs because the selection of new states is based on the nearest neighbor calculation. As RRT explores a space, the regions separated by the tree's edges become close to voroni regions of the space.

Bi-directional searching was presented in \cite{kuffner2000rrt} to significantly increase the performance of RRT. Two trees are instantiated - one at the initial state $x_{init}$ and one at the goal state $x_{goal}$. When the trees connect, the query is complete. At each iteration of the algorithm, one of the trees, $T_0$, is extended by the usual set of procedures in RRT. Let $x_{new}$ be the newly added state to $T_0$. A new procedure, $Connect$, is called to attempt to add $x_{new}$ to the second tree, $T_1$. If this connection is unsucessful, then the trees are swapped so that $T_1$ can be extended.


\subsection{Discussion}

Both sampling-based approaches sample stochastically in order to build an approximation of a C-space. They differ in the data structure used as the C-space approximation. Because each approach builds an approximation of the C-space, the basic versions are not able to find an optimal path to the goal. Several extensions have been made to these approaches to improve the optimality of the resulting paths.

For PRM, most of the immediate work following \cite{PRM} focused on the learning phase of PRM to improve its ability to capture narrow passageways in the C-space. This is because narrow passageways are the most significant challenge when using a sampling-based approach. Extensions of the PRM algorithm target one of two aspects: the node generation process or the process of finding a neighborhood of candidate connections. 

Obstacle Based PRM (OBPRM) \cite{amato1998obprm} generates nodes based on obstacle surfaces. Thus, the obstacle information guides the node generation process. It can generate both contact configurations (for manipulation planning) and configurations in free space for general path-planning. Each sampled state is translated towards an obstacle until collision is made, and then the configuration is recorded as $c_{in}$. A random direction is selected to translate $c_{in}$ on until a free configuration, $c_{out}$, is found. The contact configuration between $c_{in}$ and $c_{out}$ can be found through binary search.
Medial Axis PRM (MAPRM) \cite{wilmarth1999maprm} proposes a node generation strategy based on the medial axis of a space. Each new sampled state is retracted onto the medial axis of the robot's configuration-free space. The exact medial axis does not need to computed with this approach. Each sampled configuration is moved towards its nearest neighbor on the graph via bisection until there is no unique closest node on the graph. An approach to sampling based on Gaussian distribution was presented in \cite{boor1999gaussian}. This approach assigns probability values to C-space regions based on their distance from C-space obstacles. Higher values are given to areas closer to obstacles. Each iteration of building the graph does the following: a configuration is obtained via uniform sampling, then a distance value $d$ is chosen based on a Gaussian distribution uncertainty, and a second configuration is obtained at distance $d$ from the first sampled configuration, i.e. closer to an obstacle. All of these strategies help the planner deal with narrow passageways by generating nodes in regions close to obstacles, which are the hardest regions to plan in.

A visilibity roadmap approach \cite{nissoux1999visibility} significantly reduces the number of nodes by only keeping nodes that either connect two connected components or are not visible by any "guard" nodes in the roadmap. A node is visible by another node if they can be connected with a straight-line that lies entirely in the free portion of a robot's C-space. Guard nodes are ones that are not visible by other guard nodes. 

Significant reductions in planning time were achieved by taking a "lazy" approach \cite{bohlin2000path}. This work builds a graph without considering collision. After a shortest path is found, collision checking is performed on the path. If collision is found, then the corresponding nodes and/or edges are removed from the graph, and a new shortest path is found. This process repeats until a shortest path is found that contains no collision.


The RRT approach also received a significant amount of work to improve its overall speed and sampling. Using RRT for real-time robot navigation was presented in \cite{bruce2002real}. This work proposed the Extended RRT (ERRT) algorithm in order to use RRT on a real robot in a dynamic environment. To use the basic version of RRT in a dynamic environment, an entirely new tree must be created each time the robot needs to change trajectories to adapt to its environment. Using the intuition that prior trees contain states that may lead to the goal, the authors used a \emph{waypoint cache} to reduce the iterations needed to find the goal. A waypoint cache is a set of states that were used in a previous path. When a new path must be found, the points in the waypoint cache are used as samples. Simulation experiments show that this approach can significantly reduce planning time when the queries become complicated to solve. This work was also implemented onto the RoboCup F180 robot system, which was the first time RRT was shown to work on a real robot. 

Dynamic-Domain RRT \cite{yershova2005dynamic} is an approach to improve the node generation of RRT by limiting the size of the sampling area. If $v$ is a boundary point at some distance $R$ from a C-space obstacle, then the \emph{boundary domain} is the intersection of the Voroni region of $v$ and a sphere of radius $R$ centered at $v$. The boundary domains of each existing point on the RRT create the \emph{Dynamic Domain} for the RRT. Rather than adding nodes sampled from the entire state space, only nodes that are within the Dynamic Domain of an RRT are considered.

An Obstacle Based RRT approach \cite{tang2006obstacle} uses a group of growth operators to generate nodes that are close to obstacles. These operators use vectors between an obstacle's vertices to determine what direction to move a sample in. 

The work done in \cite{zhang2008efficient} retracts nodes with a local optimization approach. Given a sample $q_r$ that is in collision with an obstacle, the algorithm makes an initial guess $q_n$, and then creates a new sample $q_c$ in the contact space that is found between $q_r$ and $q_n$. A set of nodes, $S$, can be found in the contact space that are within a distance threshold of $q_r$. The algorithm then performs an optimization loop in order to find the node in $S$ closest to the original sample $q_r$.



Using these approaches to find an optimal path was explored in \cite{karaman2011sampling}. Three algorithms, PRM*, RRG, and RRT*, were proposed that extend the standard PRM and RRT algorithms. Analysis in this paper shows that the three algorithms presented are 1) asymptotically optimal, and 2) within a constant factor of the computational complexity of the standard sampling algorithms. 

The PRM* algorithm is only slightly different from the standard version. In the standard version of PRM, the nearest neighbors for a sample are obtained with a fixed radius of the sample. The PRM* algorithm uses a variable radius that is a function of the number of nodes in the roadmap. Let $n$ denote the number of vertices in the roadmap. The radius function used in PRM* is 

\begin{equation}
r(n) = 2(1+1/d)^{1/d}*(\mu(X_{free})/ \zeta_d)^{1/d} * (log(n)/n)^{1/d}
\end{equation}

where $d$ is the dimension of the state space $X$, $\mu(X_{free})$ is the volume of the obstacle-free space, and $\zeta_d$ is the volume of the unit ball in the $d$-dimensional Euclidean space. The radius decreases as $n$ increases and the rate of decay is proportional to $log(n)$.

Similar to PRM, the PRM* algorithm is meant to handle multiple-query problems. The \emph{Rapidly exploring Random Graph} (RRG) algorithm is more suited for single-query problems. It is an incremental algorithm that builds a connected roadmap to use for planning. It is an extension of the RRT algorithm with one key difference: when a new vertex is added to the vertex set, all vertices that already exist in the set within a certain radius are connected to the new vertex. This means that the resulting data structure is a graph rather than a tree.

RRT* modifies RRG by maintaining a tree structure rather than a graph. A subset of edges is considered redundant and removed so that cycles are not formed. These edges are ones that are not part of the shortest path from the root of the tree to a vertex.


Speaking generally, sampling-based approaches work well for solving most robot motion planning problems. However, the time it takes to rebuild roadmaps, plan a new path, and recompute a new trajectory can be too expensive to avoid obstacles of unforeseen motion. The RRT approach is the most successful in handling real-time robot motion planning because it is designed to handle single-query problems. Also, the RRT approach extends intuitively to Kinodynamic Planning (Section \ref{sec:kino}) which can significantly speed up the time to compute an entire trajectories (instead of only a path).

\input{kinodynamic}

\section{Real-time Motion Planning} \label{sec:real-time}

Real-time Motion Planning is concerned with enabling a robot to react to changes in an environment. These changes are unknown to the robot beforehand and the planning algorithms must enable the robot to adapt to these changes on the fly.

\subsection{Elastic Strips} \label{sec:elastic}

Elastic Strips \cite{brock2002elastic} is an approach to real-time motion planning in unstructured dynamic environments that leverages potential fields and a decomposition-based path-planning strategy \cite{brock2001decomposition} to produce motion that allows a robot to avoid obstacles while also moving towards a goal.

An elastic strip is a volume formed by the union of a set of homotopic paths. These paths are slight modifications to a candidate path that satisfies global constraints of a robot's task, such as moving to a goal state. The volume, also called a \emph{tunnel}, is obtained by the decomposition-based strategy in \cite{brock2001decomposition}. The volume is "elastic" because it can be slightly modified by modifying one of the homotopic paths while maintaining topological properties of the candidate path and local constraints such as obstacle avoidance. 


\subsubsection{Obstacle Avoidance}

The Elastic Strips framework employs local potential fields to modify trajectories in response to changes in the environment. The potential fields act on the future points along the robot's path.

A path is represented as a discrete set of configurations. Let $P_c$ be the candidate path for a robot. To avoid obstacles, a new candidate path, $P_c'$, must be formed by modifying $P_c$ so that $P_c'$ lies within the elastic tunnel. To modify a path, two work space forces are created from two potential fields and applied to the path's configurations. The two forces are $V_{external}$ and $V_{internal}$. The external force is based on the robot's proximity to obstacles. For a point $p$ on the robot, $V_{ext}$ is defined as

\[ \begin{cases} 
      \frac{1}{2}k_r(d_0 - d(p))^2 & \text{if}  d(p) < d_0 \\
      0 & \text{otherwise}
   \end{cases}
\]


where $d(p)$ is the distance between $p$ and the closet obstacle, $d_0$ is the region of influence around obstacles, and $k_r$ is a scalar. The resulting force from $V_{ext}$ is

\begin{equation}
\textbf{F}^{ext}_p = -\nabla V_{ext} = k_r(d_0-d(p))\frac{d}{||d||}
\end{equation}

where $d$ is the vector between $p$ and the closest point on an obstacle. This repulsive force, $\textbf{F}^{rep}_p$, will modify the existing configurations by pushing them away from obstacles. If the robot is not close to an obstacle, then the repulsive force has no effect.

If an obstacle moves away from the robot after applying a repulsive force, then the robot's path should contract in order to make the path shorter. This is achieved by an internal force generated by virtual springs at each of the robot's control points along a path. This force is meant to bring each configuration closer to its adjacent configurations on a path. The internal force acting at control point $p^i_j$ on the $j$th link at configuration $q_i$ connected to configurations $q_{i-1}$ and $q_{i+1}$ is defined as

\begin{equation}
\textbf{F}^{int}_{i,j} = k_c\left( \frac{d^{i-1}_j}{d^{i-1}_j+d^i_j} (p^{i+1}_j-p^{i-1}_j)-(p^i_j-p^{i-1}_j) \right)
\end{equation}


%\begin{equation}
%V_{external}(p) = \left\{ \frac{1}{2}k_r(d_0 - d(p))^2 \text{if d(p) < d_0}
%\end{equation}


There are two scenarios that result in a failure to find a new path. One scenario is when a topological change occurs in the robot's configuration free space. Because the modification method maintains the topological properties of the candidate path, there will not be a successful modification if a topological change occurs.

The second scenario is when a structural local minimum occurs. A structural local minima is when the configuration free space around the candidate path has become so narrow that it cannot be modified enough to avoid an obstacle. Note that this is not a local minima in the workspace of the robot. A separate planner can be employed to move through the narrow passageway.

\subsubsection{Discussion}

The Elastic Strips method was extended to the Elastic Roadmaps method \cite{yang2009elastic} in order to further reduce the necessary C-space information needed. The Elastic Roadmaps method utilizes the OBPRM method to create milestones for a robot's path, and these milestones are labelled as task-consistent, valid, or invalid based on the robot's ability to execute the path in the current environment. This approach has been shown to successfully plan a mobile manipulator with task constraints in the presence of unpredictable moving obstacles. However, it still may suffer issues moving a robot between milestones, or not be able to find a path due to the incompleteness of OBPRM.

Using potential fields as controllers as an approach to generate motion while satisfying other constraints has inspired much work for general robot motion planning. These approaches, combined with the operational space \cite{khatib1987unified}, have been most realized in planning for highly constrained motion planning, such as humanoid models that need to move toward a goal while maintaining balance and posture. These problems, however, do not generally have strict real-time requirements.

\subsection{Real-time Adaptive Motion Planning (RAMP)}
	
		
One of the key difficulties in robot motion planning is planning in high-dimensional state spaces. Sampling-based methods are able to find a path in large state spaces, but they can be far from optimal and re-building roadmaps is expensive for real-time planning. The Real-time Adaptive Motion Planning (RAMP) \cite{RAMP} framework utilizes evolutionary computation for real-time planning and execution of high-DOF robots in dynamic environments.


RAMP is a framework that utilizes evolutionary computation for real-time planning and execution in dynamic environments. Evolutionary computation is a field designed to leverage sophisticated data structures with genetic algorithms to rapidly search through a solution space \cite{michalewicz2013genetic}. They are particularly suited to approximate optimization problems. They are useful for motion planning because an evolutionary computation approach to planning allows us to drastically change a robot's trajectory very quickly and incorporate optimization into our state spaces searches. Using evolutionary computation for path planning was first proposed in \cite{EPN_Adaptive}. 

\subsubsection{Overview}\label{subseq:RAMP Overview}

A set of trajectories, called a \emph{population}, is maintained throughout the entire run time. Each trajectory in the population is a chromosome. At each \emph{generation}, 1-2 randomly selected chromosomes are modified via genetic operators. The new chromosome(s) may replace other trajectories in the population. 


The framework interweaves three separate procedures called planning cycles, control cycles, and sensing cycles. At each planning cycle (or \emph{generation}), the population of trajectories is modified by a genetic operator. Control cycles are procedures that select a trajectory from the population that will be executed by the robot and update the starting motion state of each trajectory in the population. The frequency of control cycles is adaptive based on constraints needed to satisfy in order to switch trajectories. Sensing cycles are procedures that update information about the environment.


\subsubsection{Modification}


The basic modification operators are described in \cite{EPN_Adaptive}. They are \emph{Insert}, \emph{Delete}, \emph{Swap}, \emph{Change}, and \emph{Crossover}. These modification operations are computationally inexpensive so it is possible to perform 100+ within a second. The operators can result in drastic changes to a path. This means that a new feasible path may be found in a very small amount of time, which facilitates real-time path planning that can react to changes in an environment. Modification operators can be added, removed, or replaced based on the application of the robot. For instance, a \emph{Stop} operator was introduced in \cite{RAMP} that stops the base of a mobile manipulator for a random amount of time. This operator promoted decoupling of the base and manipulator, which was desirable to exploit redundancy. 


\subsubsection{Evaluation}

Evolutionary computation utilizes an evaluation function to select the best chromosome from a population. In the RAMP framework, this corresponds to evaluating the population to choose the trajectory for a robot to execute. In RAMP, feasible trajectories are evaluated differently than infeasible trajectories. A trajectory's feasibility is determined by collision with obstacles and if all constraints can be satisfied when switching to the trajectory.

A feasible trajectory is evaluated by a weighted, normalized sum based on optimization criteria:

\begin{equation}
	Cost = \sum_{i=1}^{N} C_i\frac{V_i}{\alpha_i}
\end{equation}

where $C_i$ is a weight, $V_i$ is a value for an optimization criterion, and $\alpha_i$ is a normalization value. Common criteria are time costs, energy costs, orientation change, and manipulability costs.

Infeasible trajectories are trajectories that are in collision or cannot be executed due to not satisfying constraints. Rather than optimization criteria, they are evaluated based on penalties. For a mobile robot system, the following would be an evaluation function for infeasible trajectories:

\begin{equation}
Cost = P_T + P_\theta
\end{equation}
\begin{equation}
P_T = \frac{Q_T}{T_{coll}}
\end{equation}
\begin{equation}
P_\theta = Q_\theta\frac{\Delta\theta}{M}
\end{equation}

where $Q_T$ and $Q_θ$ are large constant values, $T_{coll}$ is the time
of the first collision in the trajectory, $\Delta\theta$ is the orientation change, and $M$ is a normalization term.

The evaluation for infeasible trajectories should be designed to differentiate which infeasible trajectories are better than others. For instance, the term $P_T$ ensures that trajectories with collision much later in time will be evaluated as better than trajectories with collision occurring very soon.

\subsubsection{Sensing}

Sensing cycles update the latest environment changes for the evaluation function. For dynamic obstacles, simple trajectories are predicted to use for collision detection in CT-space. An obstacle trajectory is predicted by assuming the sensed velocity is constant. Therefore, the predicted trajectory will be either a straight-line or circular arc. These are used for collision detection when evaluating the population. The obstacles may not follow what RAMP predicts, but future changes will be captured in future sensing cycles. Since sensing cycles occur frequently, using simple trajectories is sufficient for navigation, as shown through various experiments.


\subsubsection{Discussion}

RAMP excels when planning with high-DOF robots, such as mobile manipulators. A video of results shows several mobile manipulators being planned independently (using each other robot as dynamic obstacles) to work in a task environment.

While RAMP was shown in \cite{RAMP} to perform well with mobile manipulators, it only considered mobile bases that were capable of holonomic motion. The framework was extended to work with non-holonomic bases in \cite{mcleod2016real}. This work modified the original RAMP approach by converting holonomic trajectory segments to non-holonomic segments at real-time for the robot to execute. A method to switch trajectories with smooth curves was presented, and the control cycles were made to adapt to the robot's dynamics, rather than being fixed.

RAMP has also been applied to continuum manipulator robot models \cite{xiao2010real} and to the pursuit-evasion domain \cite{annas2009intelligent}.



\section{Collision Detection}
	
	Collision detection is the problem of determining whether one or more objects in a virtual environment intersect. Collision detection is the most computationally expensive task in robot motion planning because it has to be performed at each point in a C-space representation to determine whether or not a point lies in $C_{free}$ or $C_{obs}$. This is made even more time consuming by the complexity of the obstacles and the number of obstacles in an environment.
	
\subsection{Convex Shapes}
	
	Convex polygons are the simplest form of objects to check for collision. There are two main approaches to checking collision between such objects - the GJK algorithm and the Lin-Canny algorithm.
	
	The Gilbert-Johnston-Keerthi (GJK) algorithm \cite{gilbert1988fast} is an approach for collision detection between two convex polygons or polyhedra. Let $A$ and $B$ be two nonequal sets of vertices for convex polygons. The GJK algorithm finds a Minkowski sum of the two sets, $C = A - B = \{p_1 - p_2 | p_1 \in A \text{ and } p_2 \in B\}$. If the origin is contained in $C$, then the two objects $A$ and $B$ are in collision.
	
	It is often useful to know the distance between two objects, e.g. to maintain a certain distance from obstacles. The GJK algorithm provides a simple method to compute this. Let $C$ be the Minkowski Sum between two convex polygons. Let $q$ denote the closest point in $C$ to the origin. The length of the vector $\vec{\textbf{q}}$ from the origin to $q$ is equal to the distance between the closest pair of points on $A$ and $B$. If the two objects intersect, then the length of $\vec{\textbf{q}}$ is equal to the penetration distance of one object into another.
	
	It's important to note that the entire Minkowski Sum does not need to be computed. An alternative is to iteratively build a polygon inside the Minkowski Sum by connecting a subset of its vertices and checking to see if the origin is contained in any polygons built this way. 
	
	The Lin-Canny algorithm \cite{lin1991fast} is another approach for collision detection between two convex objects. This approach finds the closest pair of features (vertices, edges, or faces) on two objects, determines if collision exists, and uses past knowledge to speed up future queries. 
	
	Given an initial feature pair $(f_a, f_b)$ (chosen randomly or manually specified), the closest points on these features, $(p_a, p_b)$, must be computed. If these points lie in the voroni space of the other object's feature, then $f_a$ and $f_b$ are the two closest features. Otherwise, find a new set of features and repeat the processing of finding and checking the closest points. Once the closest feature pair has been found, if the distance between the features are within some distance threshold, then collision exists.
	
	If the objects move as time goes on, motion history can exploited by using the previous closest feature pair each time the Lin-Canny algorithm begins. The closest feature pair is unlikely to change unless the time between collision detection queries is very large. Therefore, using the previous closest pair will drastically reduce the amount of time spent on the query. 
	
	These algorithms can be extended to work with concave objects simply by decomposing a concave object into multiple convex objects. However, real-life objects can be extremely complicated, e.g. containing over 1 million triangles. Therefore, many approaches use a bounding volume hierarchy to represent objects.
	
\subsection{Complex Objects}

	Bounding Volumes (BV) are convex polyhedra	that encompass a set of smaller polyhedra. A hierarchy is formed by placing a BV over an entire object, then sub-dividing the object into smaller regions, and placing BVs over each smaller region. To perform collision detection, a test on the root BV is performed. If there's no collision, the computation stops. Otherwise, collision detection is performed on the BV at the next level of the hierarchy. This process continues until either 1) no collision is found at any level or 2) collision is found at the lowest levels of the hierarchies. The motivating idea behind this is that collision detection queries can be answered extremely quickly when objects are significantly far apart. 
	
	There are several types of Bounding Volumes one can use to approximate a 3D object. Sphere Trees \cite{hubbard1996approximating} use spheres to bound pieces of the objects. Sphere Trees are nice because collision can be detected between two spheres extremely quickly. Axis Aligned Bounding Boxes (AABB) \cite{edelsbrunner1981intersection} are cubes that surround a shape and the edges of the cube are parallel to the world coordinate system's axes.
	
	Object-Oriented-Box (OBB) Trees \cite{gottschalk1996obbtree} is another commonly used representation for hierarchical object approximation that are similar to AABBs, but the edges of the cube are parallel to the object's orientation.
	 
	
\subsection{Discussion}

	Collision Detection has seen significant work for the field of haptics, which requires collision detection queries to be performed at 1KHz, or 0.0001 seconds. While contemporary work is mostly aimed at haptics, motion planning also benefits from such advances in collision detection. Two examples of such work are \cite{otaduy2005sensation} and \cite{barbivc2008six}. 
	
	There are still challenges for collision detection in mobile robots as well. Mobile robots tend to follow trajectories that are curves, such as B-splines or clothoids. Even though the curves are only planar, it is still a challenging problem to compute exact collision between such curves in a practical amount of time. Most collision detection queries on higher-order curves are performed as approximations for performance reasons, such as in \cite{morken2009computing}.	
	



\section{Conclusions and Open Problems}
    
    There are several issues that stop robots from driving all around in our world. One of the biggest open problems in motion planning is constrained motion planning. Robots are subject to many types of constraints: kinematic, dynamic, and task. These constraints are imposed by the robot's physical structure and the task it is trying to complete. Each type of constraint will create a manifold in the robot's state space and a robot's path must lie entirely in that manifold. 
    
    %There have been some approaches to handle specific types of constrained motion, but an approach to handle any general constraint is still an open problem. Further, handling these constraints in a dynamic environment is even more difficult. 
    
    In particular, a good solution for real-time task constrained motion has yet to exist. Some methods have been presented be done in an offline manner \cite{stilman2007task}\cite{berenson2011task}, but maintaining these constraints while adapting to the changes in a dynamic environment is much more difficult. Currently, the Elastic Roadmaps method is only approach shown to maintain task constraints in dynamic environments, but the video results show uncluttered environments. 
    
    Dynamic environments of unknown changes are a tough challenge even without considering task constraints. Frameworks such as RAMP or Elastic Strips have had success in such environments, however, they still have struggles with very cluttered environments. Independent obstacle avoidance methods can fit well with reactive and local planners, but integrating them into global motion planning frameworks can be challenging. RAMP and Elastic Strips have obstacle avoidance methods "built in" to the global path planning approach. 
    
    
    
    
    
    %However, other motion planning Obstacle avoidance methods are generally researched independently of general motion planning frameworks, e.g. applying learning models to more accurately predict future obstacle motion. These
    
    %A significant problem is real-time task-constrained motion planning. There have been some approaches to handle task-constrained motion in an offline manner \cite{stilman2007task}\cite{berenson2011task}, but maintaining these constraints while adapting to the changes in a dynamic environment is much more difficult. Currently, the Elastic Roadmaps method is only approach shown to maintain task constraints in dynamic environments, but the video results show uncluttered environments. 
    
    %Another open issue is integrating mapping and planning through object recognition. There are many cases where a robot may need to find a goal in an unknown environment. For example, in a disaster zone, a robot may be required to simultaneously map the environment and move towards a goal, e.g. a human trapped under rubble.
    
    %Faster exploration of a robot's C-space is always desirable. The latest contribution to that area is the \emph{bur} of free C-space \cite{lacevic2016burs}, which defines a local volume that allows for more exploration using the same distance information used in sampling-based planners.
    




\bibliography{biblio}
\bibliographystyle{unsrt}

\end{document}

